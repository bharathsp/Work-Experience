# MathCo
<img width="100" height="100" alt="image" src="https://github.com/user-attachments/assets/ad6daf4a-7c30-4831-b35d-50196b6aa157" />


📅 **Date:** 13 July 2022 - 28 July 2025

👤 **Role:** Data Engineer 2
## Projects:

### **Coca Cola**: Supply and Demand Planner

<img width="200" height="100" alt="image" src="https://github.com/user-attachments/assets/3bbea12d-509e-43b1-a695-936b9f8b347b" />

#### 📅 **Date:** Nov 2024 – July 2025

#### **🛠 Tools Used** 
Azure ML Studio, Azure Synapse Analytics, Azure Data Lake Gen2, Python, PySpark, SQL, Azure DevOps, Git, Power BI

#### **🛠 Responsibilities**

TBD

#### **🏆 Key Achievements**

* 🛠️ **Designed & Developed:** End-to-end *Supply & Demand Planner* tool to automate demand forecasting and supply chain balancing across multiple bottling locations.

* 🤖 **Automated DQM Pipelines:** Used *Azure ML Studio* to detect anomalies, validate completeness, and ensure schema conformity in upstream data.

* 🔄 **Data Engineering:** Built *Azure Synapse* pipelines integrated with the *Medallion Architecture* on Azure Data Lake Gen2 for efficient ingestion, transformation, and aggregation.

* 🌐 **Front-End Integration:** Enabled planners to interact with ML-driven forecasts via *API endpoints* triggered through *Azure ML Studio scoring pipelines*.

* ⚡ **Scalable ETL & ML:** Leveraged *PySpark* for scalable ETL and model scoring, optimizing run time and resource consumption.

* 📂 **Version Control & CI/CD:** Managed workflows and ML pipelines using *Git* and *Azure DevOps*, with CI/CD pipelines for streamlined deployment.

#### **⚠ Challenges**

TBD

---

### **Project Development Life Cycle (PDLC)**: NucliOS App development

<img width="300" height="75" alt="image" src="https://github.com/user-attachments/assets/f758073b-3833-4667-af2e-977c7187c0aa" />

#### 📅 **Date:** Apr 2024 - Nov 2024

#### **🛠 Tools Used** 
NucliOS, SQL Server, Python, SQL, draw.io

#### **🛠 Responsibilities**
TBD


#### **🏆 Key Achievements**

✨ I was part of the **PDLC Project** right from the **scratch stage** – a project management tool built as an **in-house replacement for Jira**.

📊 **Planned & designed** the data model using **Star Schema** & **Snowflake Schema**, modeled via **draw\.io**.

🗄️ Used **SQL Server** as the backend database.

📑 Created **BRD** and **TRD** documents.

🐍 Built an **end-to-end tool using NucliOS** (powered by Python).

📂 The tool included **tabs** such as:

* 📌 Project Dashboard
* 👥 Team Dashboard
* 📈 Sprint Progress

⚙️ Key **features** of the tool:

* 👤 User Management
* 🔐 Role-based Access Control
* 📝 Feature / Story / Task / Bug Creation

💰 Achieved a **cost saving of \~25 Million INR** by leveraging **in-house NucliOS**, reducing dependency on external tools like **Jira** or **Azure DevOps**.

#### **⚠ Challenges**
TBD

---

### **Merck Sharp & Dohme (MSD)**: Budget Allocation Optimization

<img width="300" height="90" alt="image" src="https://github.com/user-attachments/assets/907eb7b9-13fd-4f54-af00-0867cd4daaf2" />

#### 📅 **Date:** Feb 2024 – Apr 2024

#### **🛠 Tools Used** 
Python, Databricks, SQL, Power BI, Excel, NucliOS, Jira

#### **🛠 Responsibilities**
TBD

#### **🏆 Key Achievements**

* 📊 **Budget Optimization Engine:** Developed an optimizer leveraging *greedy* and *logarithmic allocation algorithms* to maximize ROI across marketing campaigns.

* ⚡ **Data Engineering & Modeling:** Cleaned, transformed, and modeled large-scale sales and spend datasets using *PySpark* and *SQL* on *Databricks* notebooks for scalability and performance.

* 🌐 **UI & Dashboard Integration:** Integrated optimizer output with a *NucliOS-based real-time UI* and *Power BI dashboards* to enable business users to simulate and visualize budget scenarios.

* 🔮 **Scenario Simulations:** Generated *scenario-based simulations* using historical patterns and *regression-based forecast models*.

* 🤝 **Collaboration & Documentation:** Coordinated with cross-functional teams using *Jira* and documented workflows on *Confluence*.

#### **⚠ Challenges**
TBD

---

### **Walmart**: Code and Airflow DAG Optimization

<img width="300" height="70" alt="image" src="https://github.com/user-attachments/assets/86398180-591e-4edb-9842-39c027349cd7" />

#### 📅 **Date:** Aug 2023 – Jan 2024

#### **🛠 Tools Used** 
PySpark, Apache Airflow, SQL, Apache Kafka, JSON, GitHub Actions, Docker, DBeaver

#### **🛠 Responsibilities**
TBD

#### **🏆 Key Achievements**

* 🔄 **Pipeline Refactoring:** Refactored *legacy SQL-based pipelines* into *PySpark* to improve scalability and fault tolerance for both batch and streaming jobs.

* ⚡ **Airflow Optimization:** Restructured task dependencies in *Airflow DAGs*, enabling parallelism and reducing runtime by **75%** during peak retail loads.

* 📡 **Real-Time Streaming:** Engineered a *real-time ingestion pipeline* using *Apache Kafka* to handle high-velocity JSON payloads and transform them for downstream analytics.

* 🧩 **Reusable Components:** Built reusable *PySpark functions* for nested JSON parsing and schema flattening.

* 🐳 **Deployment & Automation:** Deployed workflows using *Docker* and automated testing & deployment with *GitHub Actions*.

#### **⚠ Challenges**
TBD

---

### **Abbvie**: Automated Data Quality Monitoring

<img width="300" height="70" alt="image" src="https://github.com/user-attachments/assets/fda59b95-26c2-43c5-a34e-b3ab8c8a311e" />

#### 📅 **Date:** Jan 2023 – Jul 2023

#### **🛠 Tools Used** 
Python, PySpark, LiveRamp, Google BigQuery, Tableau, AWS S3, Jupyter

#### **🛠 Responsibilities**
TBD

#### **🏆 Key Achievements**

* 🛠️ **Automated DQM System:** Developed an *end-to-end Data Quality Monitoring* system to ensure integrity, consistency, and completeness of healthcare datasets.

* 📈 **Anomaly Detection:** Implemented *statistical thresholds* and *pattern recognition* in *PySpark* and *Python* to detect anomalies across millions of patient and prescription records.

* ☁️ **Data Integration:** Ingested and transformed data from multiple third-party vendors via *LiveRamp*, staging on *AWS S3* and *BigQuery* for downstream analysis.

* 📊 **Dashboards & Alerts:** Built *auto-refresh Tableau dashboards* with trend visualizations, data quality scores, and *alert triggers* for critical drifts.

* 📂 **Versioning & Auditability:** Logged and versioned workflows using *Jupyter notebooks* and *Git* for transparent audit trails.

#### **⚠ Challenges**
TBD

### **Abbvie**: Access and Reimbursement Dashboard

<img width="300" height="70" alt="image" src="https://github.com/user-attachments/assets/fda59b95-26c2-43c5-a34e-b3ab8c8a311e" />

#### 📅 **Date:** Jan 2022 – Aug 2022

#### **🛠 Tools Used** 
Power BI, Python, SQL, PySpark, PowerPoint, DataIKU, Excel

#### **🛠 Responsibilities**
TBD

#### **🏆 Key Achievements**

* 📊 **Access & Reimbursement Dashboard:** Designed and delivered an *interactive dashboard* for stakeholders to monitor **insurance coverage, copay uptake, and payer adherence**.

* ⚡ **Real-Time Data Processing:** Consumed and processed health data streams from APIs using *DataIKU* and *PySpark*, storing intermediate outputs in structured *SQL tables*.

* 🖥️ **Power BI Analytics:** Built *Power BI dashboards* with advanced *DAX measures* to enable filtering by **region, payer type, and patient demographics**.

* 🔍 **Automated Quality Checks:** Implemented validation logic and freshness monitoring using *Python*, with **alert-based triggers** for anomalies.

* 📑 **Executive Reporting:** Supported leadership communication with *PowerPoint decks* summarizing insights for quarterly reviews.

#### **⚠ Challenges**
TBD
