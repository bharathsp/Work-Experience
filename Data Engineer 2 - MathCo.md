# MathCo
<img width="100" height="100" alt="image" src="https://github.com/user-attachments/assets/ad6daf4a-7c30-4831-b35d-50196b6aa157" />


ğŸ“… **Date:** 13 July 2022 - 28 July 2025

ğŸ‘¤ **Role:** Data Engineer 2
## Projects:

### **Coca Cola**: Supply and Demand Planner

<img width="200" height="100" alt="image" src="https://github.com/user-attachments/assets/3bbea12d-509e-43b1-a695-936b9f8b347b" />

#### ğŸ“… **Date:** Nov 2024 â€“ July 2025

#### **ğŸ›  Tools Used** 
Azure ML Studio, Azure Synapse Analytics, Azure Data Lake Gen2, Python, PySpark, SQL, Azure DevOps, Git, Power BI

#### **ğŸ›  Responsibilities**

TBD

#### **ğŸ† Key Achievements**

* ğŸ› ï¸ **Designed & Developed:** End-to-end *Supply & Demand Planner* tool to automate demand forecasting and supply chain balancing across multiple bottling locations.

* ğŸ¤– **Automated DQM Pipelines:** Used *Azure ML Studio* to detect anomalies, validate completeness, and ensure schema conformity in upstream data.

* ğŸ”„ **Data Engineering:** Built *Azure Synapse* pipelines integrated with the *Medallion Architecture* on Azure Data Lake Gen2 for efficient ingestion, transformation, and aggregation.

* ğŸŒ **Front-End Integration:** Enabled planners to interact with ML-driven forecasts via *API endpoints* triggered through *Azure ML Studio scoring pipelines*.

* âš¡ **Scalable ETL & ML:** Leveraged *PySpark* for scalable ETL and model scoring, optimizing run time and resource consumption.

* ğŸ“‚ **Version Control & CI/CD:** Managed workflows and ML pipelines using *Git* and *Azure DevOps*, with CI/CD pipelines for streamlined deployment.

#### **âš  Challenges**

TBD

---

### **Project Development Life Cycle (PDLC)**: NucliOS App development

<img width="300" height="75" alt="image" src="https://github.com/user-attachments/assets/f758073b-3833-4667-af2e-977c7187c0aa" />

#### ğŸ“… **Date:** Apr 2024 - Nov 2024

#### **ğŸ›  Tools Used** 
NucliOS, SQL Server, Python, SQL, draw.io

#### **ğŸ›  Responsibilities**
TBD


#### **ğŸ† Key Achievements**

âœ¨ I was part of the **PDLC Project** right from the **scratch stage** â€“ a project management tool built as an **in-house replacement for Jira**.

ğŸ“Š **Planned & designed** the data model using **Star Schema** & **Snowflake Schema**, modeled via **draw\.io**.

ğŸ—„ï¸ Used **SQL Server** as the backend database.

ğŸ“‘ Created **BRD** and **TRD** documents.

ğŸ Built an **end-to-end tool using NucliOS** (powered by Python).

ğŸ“‚ The tool included **tabs** such as:

* ğŸ“Œ Project Dashboard
* ğŸ‘¥ Team Dashboard
* ğŸ“ˆ Sprint Progress

âš™ï¸ Key **features** of the tool:

* ğŸ‘¤ User Management
* ğŸ” Role-based Access Control
* ğŸ“ Feature / Story / Task / Bug Creation

ğŸ’° Achieved a **cost saving of \~25 Million INR** by leveraging **in-house NucliOS**, reducing dependency on external tools like **Jira** or **Azure DevOps**.

#### **âš  Challenges**
TBD

---

### **Merck Sharp & Dohme (MSD)**: Budget Allocation Optimization

<img width="300" height="90" alt="image" src="https://github.com/user-attachments/assets/907eb7b9-13fd-4f54-af00-0867cd4daaf2" />

#### ğŸ“… **Date:** Feb 2024 â€“ Apr 2024

#### **ğŸ›  Tools Used** 
Python, Databricks, SQL, Power BI, Excel, NucliOS, Jira

#### **ğŸ›  Responsibilities**
TBD

#### **ğŸ† Key Achievements**

* ğŸ“Š **Budget Optimization Engine:** Developed an optimizer leveraging *greedy* and *logarithmic allocation algorithms* to maximize ROI across marketing campaigns.

* âš¡ **Data Engineering & Modeling:** Cleaned, transformed, and modeled large-scale sales and spend datasets using *PySpark* and *SQL* on *Databricks* notebooks for scalability and performance.

* ğŸŒ **UI & Dashboard Integration:** Integrated optimizer output with a *NucliOS-based real-time UI* and *Power BI dashboards* to enable business users to simulate and visualize budget scenarios.

* ğŸ”® **Scenario Simulations:** Generated *scenario-based simulations* using historical patterns and *regression-based forecast models*.

* ğŸ¤ **Collaboration & Documentation:** Coordinated with cross-functional teams using *Jira* and documented workflows on *Confluence*.

#### **âš  Challenges**
TBD

---

### **Walmart**: Code and Airflow DAG Optimization

<img width="300" height="70" alt="image" src="https://github.com/user-attachments/assets/86398180-591e-4edb-9842-39c027349cd7" />

#### ğŸ“… **Date:** Aug 2023 â€“ Jan 2024

#### **ğŸ›  Tools Used** 
PySpark, Apache Airflow, SQL, Apache Kafka, JSON, GitHub Actions, Docker, DBeaver

#### **ğŸ›  Responsibilities**
TBD

#### **ğŸ† Key Achievements**

* ğŸ”„ **Pipeline Refactoring:** Refactored *legacy SQL-based pipelines* into *PySpark* to improve scalability and fault tolerance for both batch and streaming jobs.

* âš¡ **Airflow Optimization:** Restructured task dependencies in *Airflow DAGs*, enabling parallelism and reducing runtime by **75%** during peak retail loads.

* ğŸ“¡ **Real-Time Streaming:** Engineered a *real-time ingestion pipeline* using *Apache Kafka* to handle high-velocity JSON payloads and transform them for downstream analytics.

* ğŸ§© **Reusable Components:** Built reusable *PySpark functions* for nested JSON parsing and schema flattening.

* ğŸ³ **Deployment & Automation:** Deployed workflows using *Docker* and automated testing & deployment with *GitHub Actions*.

#### **âš  Challenges**
TBD

---

### **Abbvie**: Automated Data Quality Monitoring

<img width="300" height="70" alt="image" src="https://github.com/user-attachments/assets/fda59b95-26c2-43c5-a34e-b3ab8c8a311e" />

#### ğŸ“… **Date:** Jan 2023 â€“ Jul 2023

#### **ğŸ›  Tools Used** 
Python, PySpark, LiveRamp, Google BigQuery, Tableau, AWS S3, Jupyter

#### **ğŸ›  Responsibilities**
TBD

#### **ğŸ† Key Achievements**

* ğŸ› ï¸ **Automated DQM System:** Developed an *end-to-end Data Quality Monitoring* system to ensure integrity, consistency, and completeness of healthcare datasets.

* ğŸ“ˆ **Anomaly Detection:** Implemented *statistical thresholds* and *pattern recognition* in *PySpark* and *Python* to detect anomalies across millions of patient and prescription records.

* â˜ï¸ **Data Integration:** Ingested and transformed data from multiple third-party vendors via *LiveRamp*, staging on *AWS S3* and *BigQuery* for downstream analysis.

* ğŸ“Š **Dashboards & Alerts:** Built *auto-refresh Tableau dashboards* with trend visualizations, data quality scores, and *alert triggers* for critical drifts.

* ğŸ“‚ **Versioning & Auditability:** Logged and versioned workflows using *Jupyter notebooks* and *Git* for transparent audit trails.

#### **âš  Challenges**
TBD

### **Abbvie**: Access and Reimbursement Dashboard

<img width="300" height="70" alt="image" src="https://github.com/user-attachments/assets/fda59b95-26c2-43c5-a34e-b3ab8c8a311e" />

#### ğŸ“… **Date:** Jan 2022 â€“ Aug 2022

#### **ğŸ›  Tools Used** 
Power BI, Python, SQL, PySpark, PowerPoint, DataIKU, Excel

#### **ğŸ›  Responsibilities**
TBD

#### **ğŸ† Key Achievements**

* ğŸ“Š **Access & Reimbursement Dashboard:** Designed and delivered an *interactive dashboard* for stakeholders to monitor **insurance coverage, copay uptake, and payer adherence**.

* âš¡ **Real-Time Data Processing:** Consumed and processed health data streams from APIs using *DataIKU* and *PySpark*, storing intermediate outputs in structured *SQL tables*.

* ğŸ–¥ï¸ **Power BI Analytics:** Built *Power BI dashboards* with advanced *DAX measures* to enable filtering by **region, payer type, and patient demographics**.

* ğŸ” **Automated Quality Checks:** Implemented validation logic and freshness monitoring using *Python*, with **alert-based triggers** for anomalies.

* ğŸ“‘ **Executive Reporting:** Supported leadership communication with *PowerPoint decks* summarizing insights for quarterly reviews.

#### **âš  Challenges**
TBD
